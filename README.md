## What this is
An early-stage research prototype exploring how molecular-level spectral data can be translated into bone-conducted audio for embodied perception.

## Input
- Molecular data (e.g. vibrational / spectral values derived from molecular structure)
- Parameter mappings extracted from molecular computation

## Processing
- Molecular parameters translated into audible frequency ranges
- Spectral values mapped to amplitude, frequency emphasis, and temporal patterns
- Audio signals generated from molecular computation rather than prerecorded sound

## Output
- Bone-conducted vibro-acoustic signals delivered through a transducer
- Perception of sound and vibration localized internally through the body

## What currently works
- Molecular data can be successfully converted into audible signals.
- Bone conduction produces a distinct embodied perception compared to air-conducted sound.

## What’s unfinished
- Perceptual calibration between different molecular mappings and user experience.
- Systematic evaluation of how molecular differences affect embodied auditory perception.

## Why it matters
This project investigates how non-human, molecular-scale information can be perceived through the body, challenging anthropocentric assumptions in human–machine interfaces.
